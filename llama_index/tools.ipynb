{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c7acb84",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051e8551",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index datasets llama-index-callbacks-arize-phoenix arize-phoenix llama-index-vector-stores-chroma llama-index-llms-ollama llama-index-embeddings-ollama llama-index-tools-google pip install llama-index-tools-mcp -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c6f5571",
   "metadata": {},
   "source": [
    "# Creating a FunctionTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93acd368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting weather for Boston\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ToolOutput(blocks=[TextBlock(block_type='text', text='The weather in Boston is ☀️')], tool_name='my_weather_tool', raw_input={'args': ('Boston',), 'kwargs': {}}, raw_output='The weather in Boston is ☀️', is_error=False)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.tools import FunctionTool\n",
    "\n",
    "\n",
    "def get_weather(location: str) -> str:\n",
    "    \"\"\"Useful for getting the weather for a given location.\"\"\"\n",
    "    print(f\"Getting weather for {location}\")\n",
    "    return f\"The weather in {location} is ☀️\"\n",
    "\n",
    "\n",
    "tool = FunctionTool.from_defaults(\n",
    "    get_weather,\n",
    "    name=\"my_weather_tool\",\n",
    "    description=\"Useful for getting the weather for a given location.\",\n",
    ")\n",
    "tool.call(\"Boston\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a51b2c",
   "metadata": {},
   "source": [
    "# Creating a QueryEngineTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbc8b635",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/hf_agent/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Load and prepare the persona dataset\n",
    "from datasets import load_dataset\n",
    "from pathlib import Path\n",
    "\n",
    "dataset = load_dataset(path=\"dvilasuero/finepersonas-v0.1-tiny\", split=\"train\")\n",
    "\n",
    "Path(\"data\").mkdir(parents=True, exist_ok=True)\n",
    "for i, persona in enumerate(dataset):\n",
    "    with open(Path(\"data\") / f\"persona_{i}.txt\", \"w\") as f:\n",
    "        f.write(persona[\"persona\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8dc70d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 documents\n"
     ]
    }
   ],
   "source": [
    "# Load documents\n",
    "from llama_index.core import SimpleDirectoryReader\n",
    "\n",
    "reader = SimpleDirectoryReader(input_dir=\"data\")\n",
    "documents = reader.load_data()\n",
    "print(f\"Loaded {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd0df78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "\n",
    "\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.tools import QueryEngineTool\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from llama_index.embeddings.ollama import OllamaEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.ingestion import IngestionPipeline\n",
    "\n",
    "# Use Ollama for embeddings and LLM\n",
    "llm = Ollama(model=\"llama3:8b\")\n",
    "embed_model = OllamaEmbedding(model_name=\"nomic-embed-text\")\n",
    "\n",
    "# Initialize ChromaDB\n",
    "db = chromadb.PersistentClient(path=\"./alfred_chroma_db\")\n",
    "chroma_collection = db.get_or_create_collection(\"alfred\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "\n",
    "\n",
    "pipeline = IngestionPipeline(\n",
    "    transformations=[\n",
    "        SentenceSplitter(chunk_size=512, chunk_overlap=50),\n",
    "        embed_model,\n",
    "    ],\n",
    "    vector_store=vector_store,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb630b85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The only persona mentioned is Persona 105.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core import VectorStoreIndex\n",
    "\n",
    "\n",
    "# Create index from vector store\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store, \n",
    "    embed_model=embed_model\n",
    ")\n",
    "\n",
    "# Create query engine\n",
    "query_engine = index.as_query_engine(llm=llm)\n",
    "\n",
    "# Create tool\n",
    "tool = QueryEngineTool.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    name=\"persona_search\",\n",
    "    description=\"Search through party attendee personas to find relevant information\",\n",
    ")\n",
    "\n",
    "# Query the tool\n",
    "response = await tool.acall(\n",
    "    \"which personas are in AI and technology?\"\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c27241",
   "metadata": {},
   "source": [
    "# Creating Toolspecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a52adbcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<llama_index.core.tools.function_tool.FunctionTool at 0x15af43280>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x15d87e470>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x15d87e620>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x15d87e500>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x15d87eb90>,\n",
       " <llama_index.core.tools.function_tool.FunctionTool at 0x15af43be0>]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.tools.google import GmailToolSpec\n",
    "\n",
    "tool_spec = GmailToolSpec()\n",
    "tool_spec_list = tool_spec.to_tool_list()\n",
    "tool_spec_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5ee3e2e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_data load_data() -> List[llama_index.core.schema.Document]\n",
      "Load emails from the user's account.\n",
      "search_messages search_messages(query: str, max_results: Optional[int] = None)\n",
      "\n",
      "        Searches email messages given a query string and the maximum number\n",
      "        of results requested by the user\n",
      "           Returns: List of relevant message objects up to the maximum number of results.\n",
      "\n",
      "        Args:\n",
      "            query (str): The user's query\n",
      "            max_results (Optional[int]): The maximum number of search results\n",
      "            to return.\n",
      "\n",
      "        \n",
      "create_draft create_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None) -> str\n",
      "\n",
      "        Create and insert a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "\n",
      "        \n",
      "update_draft update_draft(to: Optional[List[str]] = None, subject: Optional[str] = None, message: Optional[str] = None, draft_id: str = None) -> str\n",
      "\n",
      "        Update a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           This function is required to be passed a draft_id that is obtained when creating messages\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            to (Optional[str]): The email addresses to send the message to\n",
      "            subject (Optional[str]): The subject for the event\n",
      "            message (Optional[str]): The message for the event\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "\n",
      "        \n",
      "get_draft get_draft(draft_id: str = None) -> str\n",
      "\n",
      "        Get a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "\n",
      "        \n",
      "send_draft send_draft(draft_id: str = None) -> str\n",
      "\n",
      "        Sends a draft email.\n",
      "           Print the returned draft's message and id.\n",
      "           Returns: Draft object, including draft id and message meta data.\n",
      "\n",
      "        Args:\n",
      "            draft_id (str): the id of the draft to be updated\n",
      "\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[print(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
