{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad0f79c0",
   "metadata": {},
   "source": [
    "# Install the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebda2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-index llama-index-vector-stores-chroma llama-index-utils-workflow llama-index-llms-ollama pyvis -U -q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fa1be49",
   "metadata": {},
   "source": [
    "# Basic Workflow Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bdf2679b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello, world!'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import StartEvent, StopEvent, Workflow, step\n",
    "\n",
    "\n",
    "class MyWorkflow(Workflow):\n",
    "    @step\n",
    "    async def my_step(self, ev: StartEvent) -> StopEvent:\n",
    "        # do something here\n",
    "        return StopEvent(result=\"Hello, world!\")\n",
    "\n",
    "\n",
    "w = MyWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "879a674f",
   "metadata": {},
   "source": [
    "# Connecting Multiple Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fffdb7",
   "metadata": {},
   "source": [
    "We can also create multi-step workflows. Here we pass the event information between steps. Note that we can use type hinting to specify the event type and the flow of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f100b1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished processing: Step 1 complete\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4127eb4a",
   "metadata": {},
   "source": [
    "# Loops and Branches"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66e4434",
   "metadata": {},
   "source": [
    "We can also use type hinting to create branches and loops. Note that we can use the `|` operator to specify that the step can return multiple types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2f4b0c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bad thing happened\n",
      "Bad thing happened\n",
      "Bad thing happened\n",
      "Good thing happened\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: First step complete.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event\n",
    "import random\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class LoopEvent(Event):\n",
    "    loop_output: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent | LoopEvent) -> ProcessingEvent | LoopEvent:\n",
    "        if random.randint(0, 1) == 0:\n",
    "            print(\"Bad thing happened\")\n",
    "            return LoopEvent(loop_output=\"Back to step one.\")\n",
    "        else:\n",
    "            print(\"Good thing happened\")\n",
    "            return ProcessingEvent(intermediate_result=\"First step complete.\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeae51d9",
   "metadata": {},
   "source": [
    "# Drawing Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2aa3900f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "workflow_all_flows.html\n"
     ]
    }
   ],
   "source": [
    "from llama_index.utils.workflow import draw_all_possible_flows\n",
    "\n",
    "draw_all_possible_flows(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72088818",
   "metadata": {},
   "source": [
    "# State Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27eb4f78",
   "metadata": {},
   "source": [
    "Instead of passing the event information between steps, we can use the `Context` type hint to pass information between steps.\n",
    "This might be useful for long running workflows, where you want to store information between steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507bf395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the capital of France?\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Finished processing: Step 1 complete'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from llama_index.core.workflow import Event, Context\n",
    "from llama_index.core.agent.workflow import ReActAgent\n",
    "\n",
    "\n",
    "class ProcessingEvent(Event):\n",
    "    intermediate_result: str\n",
    "\n",
    "\n",
    "class MultiStepWorkflow(Workflow):\n",
    "    @step\n",
    "    async def step_one(self, ev: StartEvent, ctx: Context) -> ProcessingEvent:\n",
    "        # Process initial data\n",
    "        await ctx.store.set(\"query\", \"What is the capital of France?\")\n",
    "        return ProcessingEvent(intermediate_result=\"Step 1 complete\")\n",
    "\n",
    "    @step\n",
    "    async def step_two(self, ev: ProcessingEvent, ctx: Context) -> StopEvent:\n",
    "        # Use the intermediate result\n",
    "        query = await ctx.store.get(\"query\")\n",
    "        print(f\"Query: {query}\")\n",
    "        final_result = f\"Finished processing: {ev.intermediate_result}\"\n",
    "        return StopEvent(result=final_result)\n",
    "\n",
    "\n",
    "w = MultiStepWorkflow(timeout=10, verbose=False)\n",
    "result = await w.run()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492f4501",
   "metadata": {},
   "source": [
    "# Multi-Agent Workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "202d2be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of 5 and 3 is 8.\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.agent.workflow import AgentWorkflow, ReActAgent\n",
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "# Define some tools\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"Add two numbers.\"\"\"\n",
    "    return a + b\n",
    "\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "llm = Ollama(\n",
    "    model=\"qwen2.5-coder:7b\",  # or \"mistral\", \"phi\", \"codellama\", etc.\n",
    "    request_timeout=60.0,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# we can pass functions directly without FunctionTool -- the fn/docstring are parsed for the name/description\n",
    "multiply_agent = ReActAgent(\n",
    "    name=\"multiply_agent\",\n",
    "    description=\"Is able to multiply two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to multiply numbers.\",\n",
    "    tools=[multiply],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "addition_agent = ReActAgent(\n",
    "    name=\"add_agent\",\n",
    "    description=\"Is able to add two integers\",\n",
    "    system_prompt=\"A helpful assistant that can use a tool to add numbers.\",\n",
    "    tools=[add],\n",
    "    llm=llm,\n",
    ")\n",
    "\n",
    "# Create the workflow\n",
    "workflow = AgentWorkflow(\n",
    "    agents=[multiply_agent, addition_agent],\n",
    "    root_agent=\"multiply_agent\"\n",
    ")\n",
    "\n",
    "# Run the system\n",
    "response = await workflow.run(user_msg=\"Can you add 5 and 3?\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8547eb5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AgentOutput(response=ChatMessage(role=<MessageRole.ASSISTANT: 'assistant'>, additional_kwargs={'tool_calls': [], 'thinking': ''}, blocks=[TextBlock(block_type='text', text='The sum of 5 and 3 is 8.')]), tool_calls=[ToolCallResult(tool_name='handoff', tool_kwargs={'to_agent': 'add_agent', 'reason': \"The request involves adding two integers, which is outside my capabilities. The 'add_agent' is better equipped to handle this task.\"}, tool_id='22b69ef6-73f7-4d53-8f65-efa48cf7b9cf', tool_output=ToolOutput(blocks=[TextBlock(block_type='text', text=\"Agent add_agent is now handling the request due to the following reason: The request involves adding two integers, which is outside my capabilities. The 'add_agent' is better equipped to handle this task..\\nPlease continue with the current request.\")], tool_name='handoff', raw_input={'args': (), 'kwargs': {'to_agent': 'add_agent', 'reason': \"The request involves adding two integers, which is outside my capabilities. The 'add_agent' is better equipped to handle this task.\"}}, raw_output=\"Agent add_agent is now handling the request due to the following reason: The request involves adding two integers, which is outside my capabilities. The 'add_agent' is better equipped to handle this task..\\nPlease continue with the current request.\", is_error=False), return_direct=True)], raw={'model': 'qwen2.5-coder:7b', 'created_at': '2025-07-09T02:28:01.050808Z', 'done': True, 'done_reason': 'stop', 'total_duration': 5314661167, 'load_duration': 19059125, 'prompt_eval_count': 796, 'prompt_eval_duration': 3884000208, 'eval_count': 26, 'eval_duration': 1406633542, 'message': Message(role='assistant', content='', thinking=None, images=None, tool_calls=None), 'usage': {'prompt_tokens': 796, 'completion_tokens': 26, 'total_tokens': 822}}, current_agent_name='add_agent')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
